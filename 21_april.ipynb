{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5e931e",
   "metadata": {},
   "source": [
    "# assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db5d96",
   "metadata": {},
   "source": [
    "## Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "Euclidean distance measures the straight-line distance between two points and is sensitive to both the magnitude and direction of the data points. It works well when the relationship between features is continuous.\n",
    "\n",
    "Formula:\n",
    "\n",
    "ùëë\n",
    "(\n",
    "ùëù\n",
    ",\n",
    "ùëû\n",
    ")\n",
    "=\n",
    "‚àë\n",
    "ùëñ\n",
    "=\n",
    "1\n",
    "ùëõ\n",
    "(\n",
    "ùëù\n",
    "ùëñ\n",
    "‚àí\n",
    "ùëû\n",
    "ùëñ\n",
    ")\n",
    "2\n",
    "d(p,q)= \n",
    "i=1\n",
    "‚àë\n",
    "n\n",
    "‚Äã\n",
    " (p \n",
    "i\n",
    "‚Äã\n",
    " ‚àíq \n",
    "i\n",
    "‚Äã\n",
    " ) \n",
    "2\n",
    " \n",
    "‚Äã\n",
    " \n",
    "Manhattan distance measures the sum of the absolute differences along each dimension (often called \"city block\" distance). It is more appropriate for cases where the data points lie on grid-like structures or when the features are independent.\n",
    "\n",
    "Formula:\n",
    "\n",
    "ùëë\n",
    "(\n",
    "ùëù\n",
    ",\n",
    "ùëû\n",
    ")\n",
    "=\n",
    "‚àë\n",
    "ùëñ\n",
    "=\n",
    "1\n",
    "ùëõ\n",
    "‚à£\n",
    "ùëù\n",
    "ùëñ\n",
    "‚àí\n",
    "ùëû\n",
    "ùëñ\n",
    "‚à£\n",
    "d(p,q)= \n",
    "i=1\n",
    "‚àë\n",
    "n\n",
    "‚Äã\n",
    " ‚à£p \n",
    "i\n",
    "‚Äã\n",
    " ‚àíq \n",
    "i\n",
    "‚Äã\n",
    " ‚à£\n",
    "Impact on performance:\n",
    "\n",
    "Euclidean distance is more sensitive to outliers and works better when the features contribute equally to the overall distance.\n",
    "Manhattan distance is more robust in high-dimensional spaces or when features are distributed in grid-like patterns. It can perform better when there are outliers or when data follows axis-aligned distributions.\n",
    "The choice of metric can affect the performance of a KNN classifier or regressor depending on the nature of the data. In some cases, Manhattan distance can outperform Euclidean distance if the relationships between features are not smooth or continuous.\n",
    "\n",
    "## Q2. How do you choose the optimal value of K for a KNN classifier or regressor? What techniques can be used to determine the optimal K value?\n",
    "The optimal value of K depends on the data and can be chosen using the following techniques:\n",
    "\n",
    "Cross-validation: Split the dataset into training and validation sets, and evaluate the performance of the model for different values of K. Choose the K value that minimizes the validation error.\n",
    "Grid search: Perform an exhaustive search over a range of K values and select the best one based on performance metrics like accuracy, F1-score (for classification), or mean squared error (for regression).\n",
    "Elbow method: Plot the performance (e.g., error rate) against different K values and look for an \"elbow point\" where the error decreases rapidly initially and then levels off. The elbow represents the optimal K.\n",
    "Typically, smaller K values lead to more complex, overfitting models, while larger K values make the model simpler, potentially increasing bias.\n",
    "\n",
    "## Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "The choice of distance metric affects how the nearest neighbors are determined:\n",
    "\n",
    "Euclidean distance: Works best when features are continuous and contribute equally to the output. Use this metric when the data points are closer in Euclidean space, and their distance reflects meaningful similarities.\n",
    "Manhattan distance: Works better for high-dimensional data, categorical features, or when features are independent. It is less sensitive to outliers compared to Euclidean distance.\n",
    "Situations where you might choose one over the other:\n",
    "\n",
    "Euclidean: When data points are expected to follow smooth and continuous distributions.\n",
    "Manhattan: When data points follow grid-like patterns, or when robustness to outliers is important.\n",
    "## Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "Common hyperparameters in KNN:\n",
    "\n",
    "K (number of neighbors): Affects the complexity of the model. A smaller K makes the model more sensitive to noise, while a larger K makes it smoother and more generalized.\n",
    "Distance metric (e.g., Euclidean, Manhattan): Determines how neighbors are calculated. The choice of metric affects how proximity is defined, which impacts prediction accuracy.\n",
    "Weighting scheme: Neighbors can be uniformly weighted (equal influence) or distance-weighted (closer neighbors have more influence). Distance weighting can improve performance when nearby points are more relevant.\n",
    "Algorithm: KNN can use brute force or more efficient algorithms like KD-Trees or Ball Trees to find neighbors faster for large datasets.\n",
    "To tune these hyperparameters:\n",
    "\n",
    "Use grid search or random search to explore different combinations.\n",
    "Evaluate model performance using cross-validation.\n",
    "Consider performance metrics like accuracy, precision, recall (classification) or MSE, R-squared (regression).\n",
    "## Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "The performance of KNN improves with a larger training set since the model relies on local data to make predictions. However, a larger training set can increase the computational cost because KNN is a lazy learner, meaning it computes distances at prediction time.\n",
    "\n",
    "Techniques to optimize training set size:\n",
    "\n",
    "Sampling: Use random or stratified sampling to reduce the training set size while maintaining diversity and representativeness.\n",
    "Dimensionality reduction: Apply PCA or feature selection techniques to reduce the number of features, which effectively reduces the \"size\" of the training space.\n",
    "Efficient search: Use KD-Trees or Ball Trees for faster nearest neighbor search, especially for large datasets.\n",
    "A balance between the training set size and computational efficiency is necessary for optimal performance.\n",
    "\n",
    "## Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
    "Drawbacks:\n",
    "\n",
    "Computational cost: KNN is computationally expensive, especially with large datasets, as it requires calculating distances for every prediction.\n",
    "\n",
    "Solution: Use KD-Trees, Ball Trees, or approximate nearest neighbor techniques to reduce the search time.\n",
    "Curse of dimensionality: In high-dimensional spaces, distance measures become less meaningful, and performance degrades.\n",
    "\n",
    "Solution: Apply dimensionality reduction techniques like PCA or feature selection to reduce the number of features.\n",
    "Sensitivity to noise: KNN can be overly sensitive to noise and outliers, especially with small K values.\n",
    "\n",
    "Solution: Increase K to smooth out the predictions, use distance weighting, or apply data preprocessing techniques like outlier detection and removal.\n",
    "Feature scaling: KNN is sensitive to the scale of the features since it relies on distance metrics.\n",
    "\n",
    "Solution: Apply standardization or normalization to ensure all features contribute equally to the distance computation.\n",
    "Imbalanced data: KNN can struggle with imbalanced datasets, as the majority class tends to dominate predictions.\n",
    "\n",
    "Solution: Use techniques like oversampling, undersampling, or adjusting class weights to balance the class distribution.\n",
    "By addressing these drawbacks through feature scaling, dimensionality reduction, and efficient search techniques, the performance of KNN can be improved for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae6b93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
